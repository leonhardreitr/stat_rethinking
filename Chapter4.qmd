---
title: "Chapter4"
format: html
editor_options: 
  chunk_output_type: console
theme: cosmo
editor: 
  markdown: 
    wrap: 72
---

```{r}
library(tidyverse)
library(ggthemes)
library(patchwork)
library(brms)
library(rethinking)
library(tidybayes)

theme_set(theme_ggdist())
```

## Using Gaussian distribution

**Ontologisch**: Welt ist voller (ann√§herungsweisen) Gaussischen
Verteilungen. Viele Vorg√§nge summieren n√§mliche Fluktationen. Teil der
Exponential-Familie

**Epistomologisch**: Besonderer Zustand der Arroganz. Wenn alles was wir
√ºber eine Verteilung wissen (wollen/k√∂nnen) MW und $SD^2$ sind, dann
erf√ºllt Gaussisch die meisten Annahmen.

## A language for decsribing models

1.  Variablen angeben, Daten (Beobachtbar), Parameter (Latent)
2.  Definieren Variablen in Form andere Variablen oder Verteilungen
3.  Kombo aus Variablen und Verteilungen ergibt ein joint generative
    Modell

$$
y_i \sim Normal(\mu,\sigma) \\
\mu_i = \beta x_i \\
b \sim Normal(0,10) \\
\sigma \sim Exponential(1) \\
x_i \sim Normal(0,1)
$$

$$
W \sim Binomial(N,p)  [Likelhood]\\
p \sim Uniform(0,1) [Prior]
$$

\~ = Stochastisch. Wenn Variablen/Parameter durch eine Verteilung
definiert werden.

```{r}
n_points <- 100

d <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),
         w      = 6, 
         n      = 9) |>
  mutate(prior      = dunif(p_grid, min = 0, max = 1),
         likelihood = dbinom(w, size = n, prob = p_grid)) |>
  mutate(posterior = likelihood * prior / sum(likelihood * prior))


d |> 
  pivot_longer(prior:posterior) |> 
  mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) |> 
  ggplot(aes(p_grid, value, fill = name)) +
  geom_area() +
  scale_fill_manual(values = c("steelblue", "darkred", "blueviolet")) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(legend.position = "none") +
  facet_wrap(~ name, scales = "free") + 
  theme_calc()
```

## Gaussian Model of height

```{r}
data(Howell1)
d <- Howell1
```

Estimates = Ganze Verteilung Posterior Verteilung daher eine Verteilung
von Gaussischen Verteilungen.

```{r}
glimpse(d)

d$height

d2 <- d |> 
  filter(age >= 18)

nrow(d2)
```

```{r}
dens(d2$height)
```

Anhand Plots von roher Daten zu entscheiden, wie man sie modellieren
soll ist keine gute Entscheidung. Erstens: Kann aus mehreren
(Gaussischen) Verteilungen bestehen. Zweitens: DIE EMPIRISCHE VERTEILUNG
MUSS NICHT GAUSSISCH SEIN, DAMIT WIR EINE VERWENDEN.

$$
h_i \sim Normal(\mu, \sigma) \\
\mu = \sim(178,20) \\
\sigma \sim(0,50)
$$

```{r}
p1 <- 
  tibble(x = seq(from = 100, to = 250, by = .1)) |> 
  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 25))

p1

p2 <-
  tibble(x = seq(from = -10, to = 60, by = .1)) |> 
  ggplot(aes(x = x, y = dunif(x, 0, 50))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 10, to = 60, by = 10))
p2

n <- 1e4

set.seed(1999)

sim <- 
  tibble(sample_mu = rnorm(n, 178, 20),
         sample_sigma = runif(n,0,50)) |> 
  mutate(height = rnorm(n, sample_mu, sample_sigma))

p3 <- 
  sim |> 
  ggplot(aes(x = height)) +
  geom_density(fill = "darkseagreen", col = "white") +
  scale_x_continuous(breaks = c(0,73,178,284)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)") 
  
p3




set.seed(1999)

sim <- 
  tibble(sample_mu = rnorm(n, 178, 100),
         sample_sigma = runif(n,0,50)) |> 
  mutate(height = rnorm(n, sample_mu, sample_sigma))

# compute the values we'll use to break on our x axis
breaks <-
  c(mean(sim$height) - 3 * sd(sim$height), 0,
    mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) |> 
    round(digits = 0)

text <-
  tibble(height = 272 - 25,
         y      = .0013,
         label  = "tallest man",
         angle  = 90)

# plot
p4 <-
  sim |>
  ggplot(aes(x = height)) +
  geom_density(fill = "black", linewidth = 0) +
  geom_vline(xintercept = 0, color = "indianred") +
  geom_vline(xintercept = 272, color = "indianred", linetype = 3) +
  geom_text(data = text,
            aes(y = y, label = label, angle = angle),
            color = "indianred") +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)\nmu ~ dnorm(178, 100)") +
  theme(panel.grid = element_blank())

(p1 + xlab("mu") | p2 + xlab("sigma")) / (p3 | p4)
```

```{r}
sim |> 
  count(height < 0) |> 
  mutate(perc = 100 * n / sum(n))
```

```{r}
n <- 200

d_grid <-
  crossing(mu    = seq(from = 140, to = 160, length.out = n),
           sigma = seq(from = 4, to = 9, length.out = n))

glimpse(d_grid)

grid_function <- function(mu, sigma) {
  
  dnorm(d2$height, mean = mu, sd = sigma, log = T) |> 
    sum()
  
}

d_grid <-
  d_grid |>
  mutate(log_likelihood = map2(mu, sigma, grid_function)) |>
  unnest(log_likelihood) |>
  mutate(prior_mu    = dnorm(mu, mean = 178, sd = 20, log = T),
         prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) |>
  mutate(product = log_likelihood + prior_mu + prior_sigma) |>
  mutate(probability = exp(product - max(product)))
  
d_grid |> 
   ggplot(aes(x = mu, y = sigma, fill = probability)) + 
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "B") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme_base()
```

```{r}
set.seed(1999)

d_grid_samples <- 
  d_grid |> 
  sample_n(size = 1e4, replace = T, weight = probability)

d_grid_samples |> 
  ggplot(aes(mu, sigma)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_colorblind() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) 

d_grid_samples |> 
  ggplot(aes(x = mu, y = sigma)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples]))


d_grid_samples |> 
  pivot_longer(mu:sigma) |> 
  ggplot(aes(x = value)) + 
  geom_density(fill = "thistle1", col = 'snow') +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)

d_grid_samples |> 
  pivot_longer(mu:sigma) |> 
  group_by(name) |> 
  tidybayes::mode_hdi(value)
```

```{r}

data("Howell1")
d <- Howell1
d2 <- d[d$age >= 18,]
b4.1 <- 
  brm(data = d2, 
      family = gaussian,
      height ~ 1,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b4.01")

plot(b4.1)

print(b4.1)

b4.1$fit


summary(b4.1, prob = .89)

b4.2 <- 
  brm(
    data = d2,
    height ~ 1,
    prior = c(prior(normal(178,0.1), class = Intercept),
              prior(uniform(0,50), class = sigma, ub = 50)),
    iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = "fits/b4.2"
  )

summary(b4.2, prob = .89)
rbind(summary(b4.1)$fixed,
      summary(b4.2)$fixed)
```

```{r}
post <- as_draws_df(b4.1)

head(post,n = 10)

post |> 
  select(b_Intercept:sigma) |> 
  cor()
```

# Linear prediction

```{r}
ggplot(d2, aes(weight, height)) +
  geom_point(size = 2, shape = 1) +
  theme_base()
```

Strategie: Parameter $\mu$ (MW der GV) in eine Linearkombination
umwandeln, welche aus Pr√§diktoren und anderen Parametern besteht.

$$
h_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta(x_i-\bar{x}) \\
\alpha \sim Normal(178,20) \\
\beta \sim Normal(0,10) \\
\sigma \sim(0,50)
$$

Der MW $\mu$ ist nun kein Estimate, sondern besteht aus anderen
Parametern und Variablen. Es ist daher NICHT Stochastisch, sondern
deterministisch. Die Parameter, in unseren Fall $\alpha$ und $\beta$
sind NICHT Teil der GV. SIe dienen lediglich dazu \mu zu manipulieren.
Diese Parameter dienen dazu zu lernen. Wor√ºber ? √úber die Daten anhand
der Posterior. Wir fragen mit $\mu_i = \alpha + \beta(x_i-\bar{x})$ zwei
Fragen: - Was erwarten wir wenn $x_i = \bar{x}$? Richtig $\alpha$. Weil
wenn $x_i = \bar{x}$ gilt, dann gilt auch $mu_i = \alpha$. - Welche
Ver√§nderung erwarten wir im Outcome, wenn $x_i$ um eine Einheit steigt?
$\beta$ beantwortet dies. Auch Slope genannt. Rate of change in
expectation.

Priors: Verteilungen der unbeobachteten Variablen aka Parameter. Warum
haben wir $\beta = 0$? Wahrscheinlichkeit f√ºr \>0 oder \<0 gleich.

```{r}
set.seed(1999)

n_lines <- 100

lines <- 
  tibble(
    n = 1:n_lines,
    a = rnorm(n_lines, mean = 178, sd =20),
    b = rnorm(n_lines, mean = 0, sd = 10)) |> 
  expand_grid(weight = range(d2$weight)) |> 
  mutate(height = a + b * (weight - mean(d2$weight)))

lines |> 
  ggplot(aes(weight, height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10, col = "red3") +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0,10")


  tibble(
    n = 1:n_lines,
    a = rnorm(n_lines, mean = 178, sd =20),
    b = rlnorm(n_lines, mean = 0, sd = 1)) |> 
  expand_grid(weight = range(d2$weight)) |> 
  mutate(height = a + b * (weight - mean(d2$weight))) |> 

  ggplot(aes(weight, height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10, col = "red3") +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0,10")
```

Model

```{r}
d2 <- 
  d2 |> 
  mutate(weight_c = weight - mean(weight))


b4.3 <- 
  brm(data = d2,
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178,20), class = Intercept),
                prior(lognormal(0,1), class = b),
                prior(uniform(0,50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b4.31")

summary(b4.3)

plot(b4.3)

b4.3b <- 
  brm(data = d2, 
      family = gaussian,
      bf(height ~ a + exp(lb) * weight_c,
         a ~ 1,
         lb ~ 1,
         nl = TRUE),
      prior = c(prior(normal(178, 20), class = b, nlpar = a),
                prior(normal(0, 1), class = b, nlpar = lb),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03b")

posterior_summary(b4.3)[1:3,] |> 
  round(2)

as_draws_df(b4.3) |> 
  select(b_Intercept:sigma) |> 
  cov() |> 
  round(3)

pairs(b4.3)

labels <- 
  c(-10,0,10) + mean(d2$weight) |> round(0)

d2 |> 
  ggplot(aes(weight_c, height)) +
  geom_abline(intercept = fixef(b4.3)[1],
              slope = fixef(b4.3)[2], col = "black") +
  geom_point(shape = 1, size = 2, col = "royalblue") +
  scale_x_continuous("Weight", breaks = c(-10,0, 10), labels = labels)
  labs(x = "Weight", y = "Height")
```

```{r}
post <- as_draws_df(b4.3)

mu_at_50 <- 
  post |> 
  transmute(mu_at_50 = b_Intercept + b_weight_c * 5.01)
 
head(mu_at_50)

mu_at_50 |> 
  ggplot(aes(x = mu_at_50)) +
  geom_density(fill = "tan", col = "plum") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()


mean_hdi(mu_at_50[,1],.width = c(.89,.97))

mu_at_50 |> 
  ggplot(aes(x = mu_at_50)) +
  stat_halfeye(point_interval = mode_hdi, width = .89,
               fill = "plum2") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```

```{r}
weight_seq <- 
  tibble(weight = 25:70) |> 
  mutate(weight_c = weight - mean(d2$weight))

mu <-
  fitted(b4.3,
         summary = F,
         newdata = weight_seq) |> 
  data.frame() |> 
  set_names(25:70) |> 
  mutate(iter = 1:n())

mu <- mu |> 
  pivot_longer(-iter,
               names_to = "weight",
               values_to = "height") |> 
  mutate(weigth = as.numeric(weight))
```

```{r}
mu |> 
  ggplot(aes(x = weight, y = height)) +
  geom_point(data = mu |> filter(iter < 101), 
             color = "royalblue", alpha = .05) +
  theme_black() + theme(panel.grid = element_blank())
```

# Curves from lines oida

Polynomial Regression & B-Splines: Ein Pr√§diktor wird in mehrere
transformiert unter der Verwendung eines Parameters f√ºr das Gewicht.
B-Splines besser

*Polynomial*: Verwendet die Power (zb. quadratisch oder kubisch) einer
Variable als weiteren Pr√§diktor

```{r}
data("Howell1")
d <- Howell1

glimpse(d)

d |> 
  ggplot(aes(weight, height)) +
  geom_point(shape = 1, col = "royalblue") +
  annotate(geom = "text", x = 42, y = 100, 
           label = "This is a curve")
```

H√§ufigste Polynomiale Regression ist eine Parabola des MW: $$
\mu_i = \alpha + \beta_1x_i + \beta_2x_i^2
$$

Model einfach zu bauen, aber schwer zu kapieren

$$
h_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_1x_i + \beta_2x_i^2 \\
\alpha \sim Normal(178,20) \\
\beta_1 \sim Log-Normal(0,1) \\
\beta_2 \sim Normal(0,1) \\
\sigma \sim Uniform(0,50)
$$ Sehr schwierig f√ºr polynomiale Modelle gute Priors zu finden

```{r}
d <- 
  d |> 
  mutate(weight_s = (weight - mean(weight)) / sd(weight)) |> 
  mutate(weight_s2 = weight_s^2)

b4.5 <- brm(
  data = d, 
      family = gaussian,
      height ~ 1 + weight_s + weight_s2,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b, coef = "weight_s"),
                prior(normal(0, 1), class = b, coef = "weight_s2"),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
seed = 4, file = "fits/b4.5")

plot(b4.5, widths = c(1,2))

print(b4.5)


d <-
  d |> 
  mutate(weight_s3 = weight_s^3)

b4.6 <- brm(
  data = d, 
      family = gaussian,
      height ~ 1 + weight_s + weight_s2 + weight_s3,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b, coef = "weight_s"),
                prior(normal(0, 1), class = b, coef = "weight_s2"),
                prior(normal(0, 1), class = b, coef = "weight_s3"),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
seed = 4, file = "fits/b4.6")

plot(b4.6, widths = c(1,2))

print(b4.6)
```

splines

```{r}
data("cherry_blossoms")

d <- cherry_blossoms

d |> 
  gather() |> 
  summarise(mean = mean(value, na.rm = T),
            sd   = sd(value, na.rm = T),
            ll   = quantile(value, prob = .055, na.rm = T),
            ul   = quantile(value, prob = .945, na.rm = T), .by = key) |> 
  mutate_if(is.double, round, digits = 2)

d |> 
  ggplot(aes(x = year, y = doy)) +
  geom_point(col = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = "#4f455c")) +
  labs(x = "Year", y = "Doy")
```

B-Splines zerlegen einen pr√§diktor in kleinere Teile. Jedes Teil bekommt
dann einen Parameter. Diese verwenden wir dann in einem Linearen Modell

```{r}
d2 <- 
  d |> 
  drop_na()

num_knots <- 15
knot_list <- quantile(d2$year,
                      probs = seq(from = 0,
                                  to = 1, length.out = num_knots))


d |> 
  ggplot(aes(x = year, y = doy)) +
  geom_vline(xintercept = knot_list, col = "snow", alpha = .5) +
  geom_point(col = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = "#4f455c")) +
  labs(x = "Year", y = "Doy")
```

```{r}
library(splines)

B <- bs(d2$year,
        knots = knot_list[-c(1,num_knots)],
        degree = 3, intercept = T)

str(B)

b <-
  B |> 
  data.frame() |> 
  set_names(str_c(0, 1:9), 10:17) |> 
  bind_cols(select(d2, year)) |> 
  pivot_longer(-year,
               names_to = "bias_function",
               values_to = "bias")

b |> 
  ggplot(aes(year, bias, group = bias_function)) +
  geom_vline(xintercept = knot_list, col = "snow", alpha = .5) +
  geom_line(col = "#ffb7c5", alpha = .5, linewidth = 1.5) +
  ylab("Bias value") +
  theme_bw() +
 theme(panel.background = element_rect(fill = "#4f455c"), panel.grid = element_blank())

```

```{r}
d3 <-
  d2 |> 
  mutate(B = B) 

d3 |>  glimpse()



b4.8 <- 
  brm(data = d3,
      family = gaussian,
      doy ~ 1 + B,
      prior = c(prior(normal(100,10), class = Intercept),
                prior(normal(0,10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4, file = "fits(b4.8)")


print(b4.8)
```

# Exercises

4E1 - First line

4E2 - 2

4E3

\$\$ Pr(\mu, \sigma \|y)
=\frac{Pr(y|\mu,\sigma) Pr(\mu) Pr(\sigma)}{\int\int Pr(y|\mu,\sigma) Pr(\mu) Pr(\sigma)d\mu d\sigma}

\$\$ 4E4 Second line

4E5 3

4M1

```{r}
d <- 
  tibble(mu = rnorm(1e4, 0,10),
         sigma = rexp(1e4,1)) |> 
  mutate(y = rnorm(1e4, mu, sigma))
glimpse(d)

d |> 
  ggplot(aes(x = y)) +
  geom_density(fill = "forestgreen")
```

4M2

```{r}
brm(data = d, family = gaussian,
    formula = y_i = 1 + mu,
    prior = c(
                prior(normal(0, 10), class = b),
                prior(exponential(1), class = sigma)))
```

4M3 $$
y \sim Normal(\mu, \sigma) \\
\mu = \alpha + \beta x_i \\
\alpha \sim Normal(0,10) \\
\beta \sim Uniform(1) \\
\sigma \sim Exponential(1)
$$ 4M4 $$
Height_{ij} \sim Normal(\mu_{ij}, \sigma) \\
\mu _{ij}= \alpha + \beta (y_j - \bar{y}) \\
\alpha \sim Normal(100,10) \\
\beta \sim Normal(0,25) \\
\sigma \sim Exponential(1)
$$ \alpha = Durchschnittliches Alter im durchschnittlichen Jahr. Slope
hier sehr weit, da Wachstum vermutlich non linear ist und es etwa zu
Wachstumsch√ºben kommen k√∂nnte.

Simuliert w√§re es so:

```{r}
n <- 50

d <- tibble(
  group = seq_len(n),
   alpha = rnorm(n, 100, 10),
       beta = rnorm(n, 0, 25),
       sigma = rexp(n, 1)) |> 
  expand(nesting(group, alpha, beta, sigma), year = c(1,2,3)) |> 
  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma))

d |> 
  ggplot(aes(x = year, y = height, group = group)) +
  geom_line(col = "firebrick2") +
  labs(x = "Year", y = "Height")
```

Naja..., Negative Slopes machen in diesem Kontext eigentlich keinen
Sinn. Vermutlich sollte ich eher Log-Normal f√ºr den Prior nehmen

4M5 Ah okay, ja jetzt wei√ü ich das Jahr und Gr√∂√üe positiv
zusammenh√§ngen.

also Zeit f√ºr log-normal

$$
\beta \sim Log-Normal(1,0.5)
$$

```{r}
sample <- rlnorm(1e5, 1, 0.5)

boundaries <- mean_hdi(sample, .width = .89) # ofc .89 its a prime!

ggplot() +
  stat_function(data = tibble(x = c(0, 10)), mapping = aes(x = x),
                geom = "line", fun = dlnorm,
                args = list(meanlog = 1, sdlog = 0.5)) +
  geom_ribbon(data = tibble(x = seq(boundaries$ymin, boundaries$ymax, 0.01)),
              aes(x = x, ymin = 0, ymax = dlnorm(x, 1, 0.5)),
              alpha = 0.8, fill = "#ffb7c5") +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  labs(x = expression(beta), y = "Density")


n <- 50

d <- tibble(
  group = seq_len(n),
   alpha = rnorm(n, 100, 10),
       beta = rlnorm(n, 1, .5),
       sigma = rexp(n, 1)) |> 
  expand(nesting(group, alpha, beta, sigma), year = c(1,2,3)) |> 
  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma))

d |> 
  ggplot(aes(x = year, y = height, group = group)) +
  geom_line(col = "steelblue") +
  labs(x = "Year", y = "Height")
```

4M6 Eine Varianz von 64 w√§re eine SD von 8

```{r}
sqrt(64)
```

Wenn wir wirklich in unseren Modell daf√ºr kontrollieren wollen, m√ºssten
wir so vorghene mit dem Prior f√ºr \sigma

$$
\sigma \sim Uniform(0,8)
$$

```{r}
n <- 50

d <- tibble(
  group = seq_len(n),
   alpha = rnorm(n, 100, 10),
       beta = rlnorm(n, 1, .5),
       sigma = runif(n, 0,8)) |> 
  expand(nesting(group, alpha, beta, sigma), year = c(1,2,3)) |> 
  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma))

d |> 
  ggplot(aes(x = year, y = height, group = group)) +
  geom_line(col = "brown", alpha = .33) +
  labs(x = "Year", y = "Height")
```

4M7

```{r}
data("Howell1")
d <- Howell1
d2 <- d[d$age >= 18,]
b4.3_1 <- 
  brm(data = d2,
      family = gaussian,
      height ~ 1 + weight,
      prior = c(prior(normal(178,20), class = Intercept),
                prior(lognormal(0,1), class = b),
                prior(uniform(0,50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b4.3_1")

as_draws_df(b4.3_1) |>
  as_tibble() |> 
  select(b_Intercept, b_weight, sigma) |>
  cov() |>
  round(digits = 3)

as_draws_df(b4.3) |>
  as_tibble() |> 
  select(b_Intercept, b_weight_c, sigma) |>
  cov() |>
  round(digits = 3)
```

4M8

```{r}
data("cherry_blossoms")

d <- cherry_blossoms

d2 <- 
  d |> 
  drop_na()

num_knots <- 45
knot_list <- quantile(d2$year,
                      probs = seq(from = 0,
                                  to = 1, length.out = num_knots))
library(splines)

B <- bs(d2$year,
        knots = knot_list[-c(1,num_knots)],
        degree = 3, intercept = T)

str(B)

b <-
  B |> 
  data.frame() |> 
  set_names(str_c(0, 1:9), 10:47) |> 
  bind_cols(select(d2, year)) |> 
  pivot_longer(-year,
               names_to = "bias_function",
               values_to = "bias")

b |> 
  ggplot(aes(year, bias, group = bias_function)) +
  geom_vline(xintercept = knot_list, col = "snow", alpha = .5) +
  geom_line(col = "#ffb7c5", alpha = .5, linewidth = 1.5) +
  ylab("Bias value") +
  theme_bw() +
 theme(panel.background = element_rect(fill = "#4f455c"), panel.grid = element_blank())

b4.m8 <- 
  brm(data = d3,
      family = gaussian,
      doy ~ 1 + B,
      prior = c(prior(normal(100,10), class = Intercept),
                prior(normal(0,10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4, file = "fits(b4.m8)")

```

4H1

```{r}
data(Howell1)
d <- Howell1
d2 <- d[d$age>=18,]
m <- map( alist(
height ~ dnorm( mu , sigma ) , mu <- a + b*weight ,
a ~ dnorm( 100 , 100 ) ,
b ~ dnorm( 0 , 10 ) ,
sigma ~ dunif( 0 , 50 ) ),
data=d2 )

post <- extract.samples( m ) 
str(post)

y <- rnorm( 1e5 , post$a + post$b*46.95 , post$sigma )
mean(y)
HPDI(y,prob=0.90)
```

4H2 a)

```{r}
data("Howell1")

d <- Howell1 |> 
  filter(age < 18)

glimpse(d)

lm(height ~ weight, data = d ) |> broom::tidy()


m <- brm(height ~ 1 + weight, data = d, family = gaussian,
            prior = c(prior(normal(138, 20), class = Intercept),
                      prior(lognormal(0, 1), class = b, lb = 0),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234)
print(m)
```

F√ºr 1kg w√§chst ein Kind 2.7cm

b)  

```{r}
fit <- tibble(weight = prediction::seq_range(d$weight, 100)) |> 
  mutate(weight_c = weight - mean(d$weight)) |> 
  add_epred_draws(m) |> 
  group_by(weight) |> 
  mean_qi(.epred, .width = .89)

pred <- tibble(weight = prediction::seq_range(d$weight, 100)) %>% 
  mutate(weight_c = weight - mean(d$weight)) %>%
  add_predicted_draws(m) %>%
  group_by(weight) %>%
  mean_qi(.prediction, .width = 0.89)

ggplot(d, aes(x = weight)) +
  geom_point(aes(y = height), alpha = 0.4) +
  geom_ribbon(data = pred, aes(ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fit,
                  aes(y = .epred, ymin = .lower, ymax = .upper),
                  fill = "darkseagreen", linewidth = 1) +
  labs(x = "Weight", y = "Height")
```

Andere Probleme sind (noch) etwas zu schwierig ü§™
